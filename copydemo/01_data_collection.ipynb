{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Collection & Preprocessing\n",
    "\n",
    "**Goal:** Download market data, calculate technical indicators, and prepare features for our DQN trading agent.\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Download SPY (S&P 500 ETF) historical data from Yahoo Finance\n",
    "2. Calculate **21 technical indicators** for our agent's state space\n",
    "3. Handle missing values and validate data quality\n",
    "4. Save processed data for training\n",
    "\n",
    "## Why These Steps Matter\n",
    "\n",
    "- **Quality data** = Better agent performance\n",
    "- **Technical indicators** = Give agent market context (momentum, trend, volatility)\n",
    "- **Proper preprocessing** = Prevent data leakage and ensure realistic simulation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"✓ pandas: {pd.__version__}\")\n",
    "print(f\"✓ yfinance: {yf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Market Data\n",
    "\n",
    "We'll download **SPY** (S&P 500 ETF) data from 2000 to present.\n",
    "\n",
    "**Why SPY?**\n",
    "- Highly liquid (easy to trade)\n",
    "- Low spreads\n",
    "- Represents overall market\n",
    "- Long history available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SPY data\n",
    "print(\"Downloading SPY data from Yahoo Finance...\")\n",
    "\n",
    "ticker = \"SPY\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "# Download\n",
    "spy_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "# Convert column names to lowercase\n",
    "spy_data.columns = spy_data.columns.str.lower()\n",
    "\n",
    "# Add ticker column\n",
    "spy_data['ticker'] = ticker\n",
    "\n",
    "print(f\"\\n✓ Downloaded {len(spy_data)} days of data\")\n",
    "print(f\"  Date range: {spy_data.index[0].date()} to {spy_data.index[-1].date()}\")\n",
    "print(f\"  Columns: {list(spy_data.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "spy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price history\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(spy_data.index, spy_data['close'], linewidth=1.5)\n",
    "axes[0].set_title('SPY Closing Price (2000-2024)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[1].bar(spy_data.index, spy_data['volume'], width=1, alpha=0.6)\n",
    "axes[1].set_title('SPY Trading Volume', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Volume', fontsize=11)\n",
    "axes[1].set_xlabel('Date', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrice range: ${spy_data['close'].min():.2f} - ${spy_data['close'].max():.2f}\")\n",
    "print(f\"Average daily volume: {spy_data['volume'].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Technical Indicators\n",
    "\n",
    "We'll calculate **21 features** across 4 categories:\n",
    "\n",
    "### 1. Returns (Price Changes)\n",
    "- 1-day, 3-day, 5-day, 10-day, 20-day returns\n",
    "- *Why?* Agent needs to know recent price momentum\n",
    "\n",
    "### 2. Momentum Indicators\n",
    "- **RSI** (Relative Strength Index) - Overbought/oversold\n",
    "- **MACD** (Moving Average Convergence Divergence) - Trend strength\n",
    "- **Stochastic** - Momentum oscillator\n",
    "- *Why?* Identify trend reversals and momentum shifts\n",
    "\n",
    "### 3. Trend Indicators\n",
    "- **SMA 50/200** (Simple Moving Averages) - Long-term trend\n",
    "- **EMA 12/26** (Exponential Moving Averages) - Short-term trend\n",
    "- *Why?* Determine if we're in uptrend, downtrend, or ranging\n",
    "\n",
    "### 4. Volatility Indicators\n",
    "- **Bollinger Bands** - Price volatility bands\n",
    "- **ATR** (Average True Range) - Volatility measure\n",
    "- *Why?* Manage risk and position sizing\n",
    "\n",
    "### 5. Volume Indicators\n",
    "- **OBV** (On-Balance Volume) - Volume momentum\n",
    "- **Volume Ratio** - Relative volume\n",
    "- *Why?* Confirm price moves with volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "data = spy_data.copy()\n",
    "\n",
    "print(\"Calculating technical indicators...\\n\")\n",
    "\n",
    "# 1. RETURNS\n",
    "print(\"[1/5] Returns...\")\n",
    "for period in [1, 3, 5, 10, 20]:\n",
    "    data[f'return_{period}d'] = data['close'].pct_change(period)\n",
    "\n",
    "# 2. MOMENTUM INDICATORS\n",
    "print(\"[2/5] Momentum indicators (RSI, MACD, Stochastic)...\")\n",
    "\n",
    "# RSI\n",
    "data['rsi'] = ta.momentum.rsi(data['close'], window=14)\n",
    "data['rsi_norm'] = (data['rsi'] - 50) / 50  # Normalize to [-1, 1]\n",
    "\n",
    "# MACD\n",
    "macd = ta.trend.MACD(data['close'])\n",
    "data['macd'] = macd.macd()\n",
    "data['macd_signal'] = macd.macd_signal()\n",
    "data['macd_diff'] = macd.macd_diff()\n",
    "data['macd_norm'] = data['macd'] / data['close']  # Normalize by price\n",
    "\n",
    "# Stochastic\n",
    "stoch = ta.momentum.StochasticOscillator(data['high'], data['low'], data['close'])\n",
    "data['stoch_k'] = stoch.stoch()\n",
    "data['stoch_d'] = stoch.stoch_signal()\n",
    "\n",
    "# 3. TREND INDICATORS\n",
    "print(\"[3/5] Trend indicators (SMA, EMA)...\")\n",
    "\n",
    "# Simple Moving Averages\n",
    "data['sma_50'] = ta.trend.sma_indicator(data['close'], window=50)\n",
    "data['price_to_sma_50'] = (data['close'] / data['sma_50']) - 1\n",
    "data['sma_200'] = ta.trend.sma_indicator(data['close'], window=200)\n",
    "data['price_to_sma_200'] = (data['close'] / data['sma_200']) - 1\n",
    "data['sma_crossover'] = (data['sma_50'] > data['sma_200']).astype(int)\n",
    "\n",
    "# Exponential Moving Averages\n",
    "data['ema_12'] = ta.trend.ema_indicator(data['close'], window=12)\n",
    "data['price_to_ema_12'] = (data['close'] / data['ema_12']) - 1\n",
    "data['ema_26'] = ta.trend.ema_indicator(data['close'], window=26)\n",
    "data['price_to_ema_26'] = (data['close'] / data['ema_26']) - 1\n",
    "\n",
    "# 4. VOLATILITY INDICATORS\n",
    "print(\"[4/5] Volatility indicators (Bollinger Bands, ATR)...\")\n",
    "\n",
    "# Bollinger Bands\n",
    "bb = ta.volatility.BollingerBands(data['close'])\n",
    "data['bb_high'] = bb.bollinger_hband()\n",
    "data['bb_low'] = bb.bollinger_lband()\n",
    "data['bb_mid'] = bb.bollinger_mavg()\n",
    "data['bb_percent'] = bb.bollinger_pband()\n",
    "data['bb_width'] = bb.bollinger_wband()\n",
    "\n",
    "# ATR\n",
    "data['atr'] = ta.volatility.average_true_range(data['high'], data['low'], data['close'])\n",
    "data['atr_pct'] = data['atr'] / data['close']  # As percentage of price\n",
    "\n",
    "# 5. VOLUME INDICATORS\n",
    "print(\"[5/5] Volume indicators (OBV)...\")\n",
    "\n",
    "# On-Balance Volume\n",
    "data['obv'] = ta.volume.on_balance_volume(data['close'], data['volume'])\n",
    "data['obv_sma'] = ta.trend.sma_indicator(data['obv'], window=20)\n",
    "data['obv_std'] = data['obv'].rolling(window=20).std()\n",
    "data['obv_norm'] = (data['obv'] - data['obv_sma']) / data['obv_std']\n",
    "\n",
    "# Volume ratio\n",
    "data['volume_sma'] = ta.trend.sma_indicator(data['volume'], window=20)\n",
    "data['volume_ratio'] = data['volume'] / data['volume_sma']\n",
    "\n",
    "print(\"\\n✓ All indicators calculated!\")\n",
    "print(f\"  Total features: {len(data.columns) - 6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some key indicators\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# Recent data for better visibility\n",
    "recent_data = data.loc['2023-01-01':]\n",
    "\n",
    "# 1. Price with Bollinger Bands\n",
    "axes[0].plot(recent_data.index, recent_data['close'], label='Close', linewidth=2, color='black')\n",
    "axes[0].plot(recent_data.index, recent_data['bb_high'], label='BB Upper', linestyle='--', alpha=0.7, color='red')\n",
    "axes[0].plot(recent_data.index, recent_data['bb_low'], label='BB Lower', linestyle='--', alpha=0.7, color='green')\n",
    "axes[0].fill_between(recent_data.index, recent_data['bb_low'], recent_data['bb_high'], alpha=0.1)\n",
    "axes[0].set_title('Price with Bollinger Bands', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RSI\n",
    "axes[1].plot(recent_data.index, recent_data['rsi'], linewidth=1.5, color='purple')\n",
    "axes[1].axhline(y=70, color='red', linestyle='--', alpha=0.5, label='Overbought')\n",
    "axes[1].axhline(y=30, color='green', linestyle='--', alpha=0.5, label='Oversold')\n",
    "axes[1].axhline(y=50, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].set_title('RSI (Relative Strength Index)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('RSI')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. MACD\n",
    "axes[2].plot(recent_data.index, recent_data['macd'], label='MACD', linewidth=1.5, color='blue')\n",
    "axes[2].plot(recent_data.index, recent_data['macd_signal'], label='Signal', linewidth=1.5, color='orange')\n",
    "axes[2].bar(recent_data.index, recent_data['macd_diff'], label='Histogram', alpha=0.3, color='gray')\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[2].set_title('MACD (Moving Average Convergence Divergence)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('MACD')\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Volume Ratio\n",
    "colors = ['green' if x > 1 else 'red' for x in recent_data['volume_ratio']]\n",
    "axes[3].bar(recent_data.index, recent_data['volume_ratio'], color=colors, alpha=0.6, width=1)\n",
    "axes[3].axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Average')\n",
    "axes[3].set_title('Volume Ratio (Current vs 20-day Average)', fontsize=12, fontweight='bold')\n",
    "axes[3].set_ylabel('Ratio')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].legend(loc='upper left')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Quality Checks\n",
    "\n",
    "Before training, we need to ensure:\n",
    "- No infinite values\n",
    "- Handle NaN values properly\n",
    "- Features are on reasonable scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Data Quality Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_counts = data.isnull().sum()\n",
    "missing_pct = (missing_counts / len(data)) * 100\n",
    "\n",
    "print(\"\\nMissing values by column:\")\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing_counts,\n",
    "    'Percent': missing_pct\n",
    "}).query('Missing > 0').sort_values('Missing', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "    print(f\"\\nTotal rows with any missing: {data.isnull().any(axis=1).sum()}\")\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_counts = np.isinf(data.select_dtypes(include=[np.number])).sum()\n",
    "if inf_counts.sum() > 0:\n",
    "    print(f\"\\n⚠ Warning: {inf_counts.sum()} infinite values found\")\n",
    "    print(inf_counts[inf_counts > 0])\n",
    "else:\n",
    "    print(\"\\n✓ No infinite values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop initial rows with NaN (from indicator calculations)\n",
    "print(\"\\nCleaning data...\")\n",
    "print(f\"  Before: {len(data)} rows\")\n",
    "\n",
    "data_clean = data.dropna()\n",
    "\n",
    "print(f\"  After: {len(data_clean)} rows\")\n",
    "print(f\"  Removed: {len(data) - len(data_clean)} rows ({((len(data) - len(data_clean))/len(data)*100):.1f}%)\")\n",
    "\n",
    "# Replace any remaining inf with NaN and drop\n",
    "data_clean = data_clean.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "print(f\"\\n✓ Clean data: {len(data_clean)} rows\")\n",
    "print(f\"  Date range: {data_clean.index[0].date()} to {data_clean.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Statistics\n",
    "\n",
    "Let's look at the distribution of our features to ensure they're reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 21 features we'll use for the agent\n",
    "feature_cols = [\n",
    "    # Returns\n",
    "    'return_1d', 'return_5d', 'return_10d',\n",
    "    # Momentum\n",
    "    'rsi', 'rsi_norm', 'macd', 'macd_signal', 'macd_diff',\n",
    "    # Trend\n",
    "    'sma_50', 'sma_200', 'sma_crossover', 'ema_12', 'ema_26',\n",
    "    # Volatility\n",
    "    'bb_high', 'bb_low', 'bb_width', 'bb_percent', 'atr', 'atr_pct',\n",
    "    # Volume\n",
    "    'volume_ratio', 'obv'\n",
    "]\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Feature Statistics (21 features for agent)\")\n",
    "print(\"=\" * 80)\n",
    "stats = data_clean[feature_cols].describe()\n",
    "print(stats.T[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "corr = data_clean[feature_cols].corr()\n",
    "sns.heatmap(corr, cmap='RdBu_r', center=0, square=True, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr = []\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i+1, len(corr.columns)):\n",
    "        if abs(corr.iloc[i, j]) > 0.8:\n",
    "            high_corr.append((corr.columns[i], corr.columns[j], corr.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"\\nHighly correlated feature pairs (|corr| > 0.8):\")\n",
    "    for f1, f2, corr_val in high_corr:\n",
    "        print(f\"  {f1} <-> {f2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"\\n✓ No highly correlated features (all |corr| <= 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Processed Data\n",
    "\n",
    "Save the clean, feature-engineered data for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('demo_data')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save to parquet (efficient format)\n",
    "output_path = output_dir / 'featured_data.parquet'\n",
    "data_clean.to_parquet(output_path)\n",
    "\n",
    "print(f\"✓ Data saved to: {output_path}\")\n",
    "print(f\"  Rows: {len(data_clean):,}\")\n",
    "print(f\"  Columns: {len(data_clean.columns)}\")\n",
    "print(f\"  Features for agent: {len(feature_cols)}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Also save feature list for reference\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'mean': data_clean[feature_cols].mean(),\n",
    "    'std': data_clean[feature_cols].std(),\n",
    "    'min': data_clean[feature_cols].min(),\n",
    "    'max': data_clean[feature_cols].max()\n",
    "})\n",
    "feature_info.to_csv(output_dir / 'feature_info.csv', index=False)\n",
    "print(f\"\\n✓ Feature info saved to: {output_dir / 'feature_info.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "✅ Downloaded 20+ years of SPY historical data  \n",
    "✅ Calculated **21 technical indicators** across 5 categories  \n",
    "✅ Cleaned data (removed NaN and inf values)  \n",
    "✅ Validated feature quality and correlations  \n",
    "✅ Saved processed data for training  \n",
    "\n",
    "### Data Summary\n",
    "\n",
    "- **Asset:** SPY (S&P 500 ETF)\n",
    "- **Time Period:** 2000-2024 (~25 years)\n",
    "- **Total Rows:** ~6,000 trading days\n",
    "- **Features:** 21 technical indicators\n",
    "- **Data Quality:** Clean, no missing/infinite values\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**→ Notebook 02:** Cross-Validation & Data Splits  \n",
    "We'll split this data into train/validation/test sets properly for time-series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
