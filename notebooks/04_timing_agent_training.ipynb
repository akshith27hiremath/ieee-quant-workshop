{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TimingAgent Training with DQN (Phase 2)\n",
    "\n",
    "This notebook trains a Deep Q-Network (DQN) agent for single-asset market timing on SPY.\n",
    "\n",
    "## Objectives:\n",
    "1. Load featured data and create TimingEnv\n",
    "2. Initialize and train DQN agent\n",
    "3. Evaluate on validation set\n",
    "4. Compare against baseline strategies\n",
    "5. Analyze learned policy\n",
    "\n",
    "**Key Innovation**: RL agent learns optimal entry/exit timing with transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nsys.path.append('..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport torch\nwarnings.filterwarnings('ignore')\n\n# RL imports\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.callbacks import EvalCallback\n\n# Project imports\nfrom src.utils.config import ConfigLoader\nfrom src.environments.timing_env import TimingEnv\nfrom src.backtesting.baselines import BuyAndHold, SMAcrossover, compare_baselines\n\n# Style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\n# Check device availability\nif torch.xpu.is_available():\n    device = 'xpu'\n    print(f\"✓ Using Intel XPU: {torch.xpu.get_device_name(0)}\")\nelse:\n    device = 'cpu'\n    print(\"✓ Using CPU (XPU not available)\")\n\nprint(\"✓ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.config:Loaded config: timing_config\n",
      "INFO:src.utils.config:Loaded config: cv_config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimingAgent Configuration:\n",
      "  Algorithm: DQN\n",
      "  Reward type: sharpe\n",
      "  Initial cash: $100,000\n",
      "  Transaction costs: {'commission_bps': 10, 'slippage_bps': 5}\n",
      "\n",
      "Training:\n",
      "  Total timesteps: 100,000\n",
      "  Eval frequency: 5,000\n",
      "\n",
      "✓ Loaded featured data: (19452, 45)\n",
      "  Filtered to SPY: (4863, 45)\n",
      "  Date range: 2005-09-02 00:00:00 to 2024-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load configs\n",
    "config_loader = ConfigLoader('../config')\n",
    "timing_config = config_loader.load('timing_config')\n",
    "cv_config = config_loader.load('cv_config')\n",
    "\n",
    "print(\"TimingAgent Configuration:\")\n",
    "print(f\"  Algorithm: {timing_config['agent']['algorithm']}\")\n",
    "print(f\"  Reward type: {timing_config['environment']['reward_type']}\")\n",
    "print(f\"  Initial cash: ${timing_config['environment']['initial_cash']:,}\")\n",
    "print(f\"  Transaction costs: {timing_config['environment']['transaction_costs']}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Total timesteps: {timing_config['training']['total_timesteps']:,}\")\n",
    "print(f\"  Eval frequency: {timing_config['training']['eval_freq']:,}\")\n",
    "\n",
    "# Load featured data\n",
    "data = pd.read_parquet('../data/features/featured_data.parquet')\n",
    "print(f\"\\n✓ Loaded featured data: {data.shape}\")\n",
    "\n",
    "# Filter to SPY\n",
    "ticker = timing_config['data']['ticker']\n",
    "if 'ticker' in data.columns:\n",
    "    data = data[data['ticker'] == ticker].copy()\n",
    "    print(f\"  Filtered to {ticker}: {data.shape}\")\n",
    "\n",
    "print(f\"  Date range: {data.index.min()} to {data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:\n",
      "  Train: 2,850 rows (2005-09-02 00:00:00 to 2016-12-28 00:00:00)\n",
      "  Val:   504 rows (2016-12-29 00:00:00 to 2018-12-31 00:00:00)\n",
      "  Test:  Held out until 2019-01-01\n"
     ]
    }
   ],
   "source": [
    "# Split data (exclude test set)\n",
    "test_start = pd.Timestamp(cv_config['test_set']['start_date'])\n",
    "train_val_data = data[data.index < test_start].copy()\n",
    "\n",
    "# Simple 85/15 split for initial training\n",
    "train_size = int(len(train_val_data) * 0.85)\n",
    "train_data = train_val_data.iloc[:train_size]\n",
    "val_data = train_val_data.iloc[train_size:]\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"  Train: {len(train_data):,} rows ({train_data.index.min()} to {train_data.index.max()})\")\n",
    "print(f\"  Val:   {len(val_data):,} rows ({val_data.index.min()} to {val_data.index.max()})\")\n",
    "print(f\"  Test:  Held out until {test_start.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Create Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.environments.timing_env:TimingEnv initialized: 17 features, 3 actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing features: ['sma_20', 'bb_upper', 'bb_lower', 'bb_pct']\n",
      "Using 17 features:\n",
      "   1. return_1d\n",
      "   2. return_5d\n",
      "   3. return_10d\n",
      "   4. rsi\n",
      "   5. rsi_norm\n",
      "   6. macd\n",
      "   7. macd_signal\n",
      "   8. macd_diff\n",
      "   9. sma_50\n",
      "  10. sma_crossover\n",
      "  11. ema_12\n",
      "  12. ema_26\n",
      "  13. bb_width\n",
      "  14. atr\n",
      "  15. atr_pct\n",
      "  16. volume_ratio\n",
      "  17. obv\n",
      "\n",
      "✓ Created TimingEnv\n",
      "  Observation space: (17,)\n",
      "  Action space: Discrete(3)\n",
      "  Actions: 0=Hold, 1=Long, 2=Short\n"
     ]
    }
   ],
   "source": [
    "# Get feature list\n",
    "features = timing_config['data']['features']\n",
    "\n",
    "# Validate features exist\n",
    "missing_features = [f for f in features if f not in train_data.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    features = [f for f in features if f in train_data.columns]\n",
    "\n",
    "print(f\"Using {len(features)} features:\")\n",
    "for i, f in enumerate(features, 1):\n",
    "    print(f\"  {i:2d}. {f}\")\n",
    "\n",
    "# Create training environment\n",
    "train_env = TimingEnv(\n",
    "    data=train_data,\n",
    "    config=timing_config['environment'],\n",
    "    features=features\n",
    ")\n",
    "\n",
    "# Wrap in Monitor\n",
    "train_env = Monitor(train_env, './logs/timing_agent/train')\n",
    "\n",
    "print(f\"\\n✓ Created TimingEnv\")\n",
    "print(f\"  Observation space: {train_env.observation_space.shape}\")\n",
    "print(f\"  Action space: Discrete({train_env.action_space.n})\")\n",
    "print(f\"  Actions: 0=Hold, 1=Long, 2=Short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Test Environment (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing environment with 100 random actions...\n",
      "\n",
      "Random agent results:\n",
      "  Steps: 100\n",
      "  Episode reward: -6867.68\n",
      "  Final portfolio value: $77,413.58\n",
      "  Total trades: 73\n",
      "\n",
      "Random Policy Performance:\n",
      "  Total return: -22.59%\n",
      "  Sharpe ratio: -6.70\n",
      "  Max drawdown: 22.59%\n",
      "\n",
      "✓ Environment works correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test environment with random actions\n",
    "print(\"Testing environment with 100 random actions...\\n\")\n",
    "\n",
    "obs, info = train_env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "for step in range(100):\n",
    "    action = train_env.action_space.sample()  # Random action\n",
    "    obs, reward, done, truncated, info = train_env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Random agent results:\")\n",
    "print(f\"  Steps: {step + 1}\")\n",
    "print(f\"  Episode reward: {episode_reward:.2f}\")\n",
    "print(f\"  Final portfolio value: ${info['portfolio_value']:,.2f}\")\n",
    "print(f\"  Total trades: {info['total_trades']}\")\n",
    "\n",
    "# Get episode stats\n",
    "stats = train_env.env.get_episode_stats()\n",
    "print(f\"\\nRandom Policy Performance:\")\n",
    "print(f\"  Total return: {stats['total_return']:.2%}\")\n",
    "print(f\"  Sharpe ratio: {stats['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max drawdown: {stats['max_drawdown']:.2%}\")\n",
    "\n",
    "print(\"\\n✓ Environment works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Create DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Create evaluation environment\nval_env = TimingEnv(\n    data=val_data,\n    config=timing_config['environment'],\n    features=features\n)\nval_env = Monitor(val_env, './logs/timing_agent/eval')\n\n# Create DQN agent\nagent_config = timing_config['agent']\n\nmodel = DQN(\n    policy=agent_config['policy'],\n    env=train_env,\n    learning_rate=agent_config['learning_rate'],\n    buffer_size=agent_config['buffer_size'],\n    learning_starts=agent_config['learning_starts'],\n    batch_size=agent_config['batch_size'],\n    tau=agent_config['tau'],\n    gamma=agent_config['gamma'],\n    train_freq=agent_config['train_freq'],\n    gradient_steps=agent_config['gradient_steps'],\n    exploration_fraction=agent_config['exploration_fraction'],\n    exploration_initial_eps=agent_config['exploration_initial_eps'],\n    exploration_final_eps=agent_config['exploration_final_eps'],\n    target_update_interval=agent_config['target_update_interval'],\n    tensorboard_log=agent_config['tensorboard_log'],\n    device=device,  # Use XPU if available\n    verbose=1\n)\n\nprint(\"✓ Created DQN agent\")\nprint(f\"  Policy: {agent_config['policy']}\")\nprint(f\"  Device: {device}\")\nprint(f\"  Learning rate: {agent_config['learning_rate']}\")\nprint(f\"  Buffer size: {agent_config['buffer_size']:,}\")\nprint(f\"  Network architecture: {agent_config['policy_kwargs']['net_arch']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Train the Agent\n",
    "\n",
    "**Note**: This will take 5-15 minutes depending on your system.\n",
    "\n",
    "You can monitor training progress in TensorBoard:\n",
    "```bash\n",
    "tensorboard --logdir=./logs/timing_agent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Total timesteps: 100,000\n",
      "This may take 5-15 minutes...\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal timesteps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiming_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_timesteps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may take 5-15 minutes...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtiming_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\rlquant\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:272\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    265\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    271\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\rlquant\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:321\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[0;32m    314\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[1;32m--> 321\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\rlquant\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:304\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, VectorizedActionNoise)\n\u001b[0;32m    301\u001b[0m ):\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;241m=\u001b[39m VectorizedActionNoise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\rlquant\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:431\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_logger:\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[0;32m    434\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_callback(callback, progress_bar)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\rlquant\\lib\\site-packages\\stable_baselines3\\common\\utils.py:287\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[1;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    284\u001b[0m save_path, format_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to log data to tensorboard but tensorboard is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m SummaryWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     latest_run_id \u001b[38;5;241m=\u001b[39m get_latest_run_id(tensorboard_log, tb_log_name)\n",
      "\u001b[1;31mImportError\u001b[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "# Create evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path='./models/timing_agent/best',\n",
    "    log_path='./logs/timing_agent/eval',\n",
    "    eval_freq=timing_config['training']['eval_freq'],\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"Total timesteps: {timing_config['training']['total_timesteps']:,}\")\n",
    "print(\"This may take 5-15 minutes...\\n\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=timing_config['training']['total_timesteps'],\n",
    "    callback=eval_callback,\n",
    "    log_interval=100,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# Save final model\n",
    "model.save('./models/timing_agent/final_model')\n",
    "print(\"✓ Saved final model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Load Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from training\n",
    "best_model = DQN.load('./models/timing_agent/best/best_model')\n",
    "print(\"✓ Loaded best model (based on validation performance)\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set (10 episodes)...\\n\")\n",
    "\n",
    "val_rewards = []\n",
    "val_stats = []\n",
    "\n",
    "for ep in range(10):\n",
    "    obs, info = val_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        action, _ = best_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = val_env.step(action)\n",
    "        episode_reward += reward\n",
    "    \n",
    "    stats = val_env.env.get_episode_stats()\n",
    "    val_rewards.append(episode_reward)\n",
    "    val_stats.append(stats)\n",
    "    \n",
    "    print(f\"Episode {ep+1:2d}: Return={stats['total_return']:+.2%}, Sharpe={stats['sharpe_ratio']:.2f}, Trades={stats['total_trades']}\")\n",
    "\n",
    "# Aggregate results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DQN Agent Validation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean reward: {np.mean(val_rewards):.2f} ± {np.std(val_rewards):.2f}\")\n",
    "print(f\"Mean return: {np.mean([s['total_return'] for s in val_stats]):.2%}\")\n",
    "print(f\"Mean Sharpe: {np.mean([s['sharpe_ratio'] for s in val_stats]):.2f}\")\n",
    "print(f\"Mean max DD: {np.mean([s['max_drawdown'] for s in val_stats]):.2%}\")\n",
    "print(f\"Mean trades: {np.mean([s['total_trades'] for s in val_stats]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Compare with Baseline Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline strategies on validation set\n",
    "print(\"Running baseline strategies on validation set...\\n\")\n",
    "\n",
    "baseline_results = compare_baselines(val_data)\n",
    "\n",
    "print(\"Baseline Strategy Results:\")\n",
    "print(baseline_results.to_string(index=False))\n",
    "\n",
    "# Add DQN results\n",
    "dqn_results = pd.DataFrame([{\n",
    "    'strategy': 'DQN Agent',\n",
    "    'total_return': np.mean([s['total_return'] for s in val_stats]),\n",
    "    'sharpe_ratio': np.mean([s['sharpe_ratio'] for s in val_stats]),\n",
    "    'max_drawdown': np.mean([s['max_drawdown'] for s in val_stats]),\n",
    "    'total_trades': np.mean([s['total_trades'] for s in val_stats]),\n",
    "    'final_value': timing_config['environment']['initial_cash'] * (1 + np.mean([s['total_return'] for s in val_stats]))\n",
    "}])\n",
    "\n",
    "all_results = pd.concat([baseline_results, dqn_results], ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON: DQN vs Baselines\")\n",
    "print(f\"{'='*80}\")\n",
    "print(all_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single episode to get detailed trajectory\n",
    "obs, info = val_env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "portfolio_values = []\n",
    "actions_taken = []\n",
    "prices = []\n",
    "\n",
    "while not (done or truncated):\n",
    "    action, _ = best_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = val_env.step(action)\n",
    "    \n",
    "    portfolio_values.append(info['portfolio_value'])\n",
    "    actions_taken.append(action)\n",
    "    prices.append(info['current_price'])\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Portfolio Value\n",
    "axes[0].plot(portfolio_values, linewidth=2, color='darkgreen', label='DQN Agent')\n",
    "axes[0].axhline(timing_config['environment']['initial_cash'], color='gray', linestyle='--', alpha=0.5, label='Initial Capital')\n",
    "axes[0].set_title('Portfolio Value Over Time', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Portfolio Value ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SPY Price\n",
    "axes[1].plot(prices, linewidth=1.5, color='steelblue', label='SPY Price')\n",
    "axes[1].set_title('SPY Price', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actions Taken\n",
    "action_colors = ['gray', 'green', 'red']  # Hold, Long, Short\n",
    "action_labels = ['Hold', 'Long', 'Short']\n",
    "for action_val in [0, 1, 2]:\n",
    "    action_steps = [i for i, a in enumerate(actions_taken) if a == action_val]\n",
    "    if action_steps:\n",
    "        axes[2].scatter(action_steps, [action_val] * len(action_steps), \n",
    "                       c=action_colors[action_val], label=action_labels[action_val],\n",
    "                       alpha=0.6, s=20)\n",
    "\n",
    "axes[2].set_title('Agent Actions', fontweight='bold', fontsize=14)\n",
    "axes[2].set_ylabel('Action')\n",
    "axes[2].set_xlabel('Time Step')\n",
    "axes[2].set_yticks([0, 1, 2])\n",
    "axes[2].set_yticklabels(['Hold', 'Long', 'Short'])\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Action distribution\n",
    "action_counts = pd.Series(actions_taken).value_counts().sort_index()\n",
    "print(\"\\nAction Distribution:\")\n",
    "for action_val, count in action_counts.items():\n",
    "    pct = count / len(actions_taken) * 100\n",
    "    print(f\"  {action_labels[action_val]:6s}: {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Performance Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison bar charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Total Return\n",
    "axes[0].bar(all_results['strategy'], all_results['total_return'] * 100, \n",
    "           color=['steelblue', 'orange', 'lightcoral', 'darkgreen'])\n",
    "axes[0].set_title('Total Return', fontweight='bold')\n",
    "axes[0].set_ylabel('Return (%)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sharpe Ratio\n",
    "axes[1].bar(all_results['strategy'], all_results['sharpe_ratio'], \n",
    "           color=['steelblue', 'orange', 'lightcoral', 'darkgreen'])\n",
    "axes[1].set_title('Sharpe Ratio', fontweight='bold')\n",
    "axes[1].set_ylabel('Sharpe Ratio')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Max Drawdown\n",
    "axes[2].bar(all_results['strategy'], all_results['max_drawdown'] * 100, \n",
    "           color=['steelblue', 'orange', 'lightcoral', 'darkgreen'])\n",
    "axes[2].set_title('Max Drawdown', fontweight='bold')\n",
    "axes[2].set_ylabel('Drawdown (%)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].invert_yaxis()  # Lower is better\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✓ Created and tested TimingEnv trading environment\n",
    "2. ✓ Initialized DQN agent with stable-baselines3\n",
    "3. ✓ Trained agent on SPY historical data\n",
    "4. ✓ Evaluated on validation set\n",
    "5. ✓ Compared against Buy & Hold and SMA Crossover baselines\n",
    "6. ✓ Visualized learned policy and performance\n",
    "\n",
    "**Key Findings**:\n",
    "- DQN agent learns non-random policy (check action distribution)\n",
    "- Risk-adjusted returns compared to baselines (Sharpe ratio)\n",
    "- Transaction costs significantly impact profitability\n",
    "- Agent behavior makes intuitive sense (or needs debugging!)\n",
    "\n",
    "**Next Steps**:\n",
    "1. **Hyperparameter Tuning**: Try different learning rates, buffer sizes, reward functions\n",
    "2. **Longer Training**: Increase total_timesteps to 500k-1M\n",
    "3. **Advanced Rewards**: Experiment with Sortino or Drawdown-aware rewards\n",
    "4. **Walk-Forward Training**: Train on multiple CV folds for robustness\n",
    "5. **Test Set Evaluation**: Final evaluation on held-out 2019-2024 data\n",
    "6. **Phase 3**: Move to PortfolioAgent with continuous actions!\n",
    "\n",
    "**Model Artifacts**:\n",
    "- Best model: `./models/timing_agent/best/best_model.zip`\n",
    "- Final model: `./models/timing_agent/final_model.zip`\n",
    "- Logs: `./logs/timing_agent/`\n",
    "\n",
    "Load saved model later:\n",
    "```python\n",
    "from stable_baselines3 import DQN\n",
    "model = DQN.load('./models/timing_agent/best/best_model')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}